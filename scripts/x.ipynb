{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a3a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candles (24197, 7)\n",
      "candles_2 (1745, 7)\n",
      "news (25425, 4)\n",
      "news_2 (2030, 3)\n",
      "‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\n",
      "üîÑ –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏...\n",
      "‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!\n",
      "[h=1d] Fold 1: RMSE=0.0217\n",
      "[h=1d] Fold 2: RMSE=0.0210\n",
      "[h=1d] Fold 3: RMSE=0.0220\n",
      "[h=1d] Fold 4: RMSE=0.0220\n",
      "[h=1d] Fold 5: RMSE=0.0257\n",
      "=== Horizon 1d: CV mean RMSE=0.0225 ¬± 0.0016 ===\n",
      "[h=20d] Fold 1: RMSE=0.0985\n",
      "[h=20d] Fold 2: RMSE=0.0964\n",
      "[h=20d] Fold 3: RMSE=0.0956\n",
      "[h=20d] Fold 4: RMSE=0.1029\n",
      "[h=20d] Fold 5: RMSE=0.1136\n",
      "=== Horizon 20d: CV mean RMSE=0.1014 ¬± 0.0066 ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No data available for prediction date 2025-09-09 00:00:00",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 736\u001b[0m\n\u001b[0;32m    733\u001b[0m fit(candles, news_features, split_date\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2024-09-08\u001b[39m\u001b[38;5;124m'\u001b[39m, model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;66;03m# predict_on_cutoff(candles, news_features, model_path='model.pkl', output_path='submission.csv')\u001b[39;00m\n\u001b[1;32m--> 736\u001b[0m results \u001b[38;5;241m=\u001b[39m predict_for_date(\n\u001b[0;32m    737\u001b[0m     candles_df\u001b[38;5;241m=\u001b[39mcandles,\n\u001b[0;32m    738\u001b[0m     news_df\u001b[38;5;241m=\u001b[39mnews_features, \n\u001b[0;32m    739\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    740\u001b[0m     prediction_date\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2025-09-09\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    741\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    742\u001b[0m )\n",
      "Cell \u001b[1;32mIn[3], line 700\u001b[0m, in \u001b[0;36mpredict_for_date\u001b[1;34m(candles_df, news_df, model_path, prediction_date, output_path)\u001b[0m\n\u001b[0;32m    697\u001b[0m df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m prediction_date]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 700\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data available for prediction date \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction_date\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    702\u001b[0m df \u001b[38;5;241m=\u001b[39m create_features(df)\n\u001b[0;32m    703\u001b[0m df \u001b[38;5;241m=\u001b[39m add_news_features_by_ticker(df, news_df, aliases)\n",
      "\u001b[1;31mValueError\u001b[0m: No data available for prediction date 2025-09-09 00:00:00"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "import yake\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import warnings\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "from catboost import CatBoostRegressor\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "candles = pd.read_csv('../data/new/forecast_data/candles.csv')\n",
    "# print('candles', candles.shape)\n",
    "\n",
    "candles_2 = pd.read_csv('../data/new/forecast_data/candles_2.csv')\n",
    "# print('candles_2', candles_2.shape)\n",
    "\n",
    "news = pd.read_csv('../data/new/forecast_data/news.csv')\n",
    "# print('news', news.shape)\n",
    "\n",
    "news_2 = pd.read_csv('../data/new/forecast_data/news_2.csv')\n",
    "# print('news_2', news_2.shape)\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "\n",
    "class MinimalTickerExtractor:\n",
    "    def __init__(self, ticker_mapping: Dict[str, str]):\n",
    "        self.ticker_mapping = ticker_mapping\n",
    "        self.all_tickers = list(ticker_mapping.values())\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º\n",
    "        self.search_dict = {}\n",
    "        for company, ticker in ticker_mapping.items():\n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ\n",
    "            self.search_dict[company.lower()] = ticker\n",
    "            \n",
    "            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã\n",
    "            variants = {\n",
    "                '—Å–±–µ—Ä–±–∞–Ω–∫': ['—Å–±–µ—Ä', '–±–∞–Ω–∫', '—Ñ–∏–Ω–∞–Ω—Å—ã', '–∫—Ä–µ–¥–∏—Ç', '–∏–ø–æ—Ç–µ–∫–∞', 'sberbank'],\n",
    "                '–≥–∞–∑–ø—Ä–æ–º': ['–≥–∞–∑', '—Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥', '–≥–∞–∑–æ–¥–æ–±—ã—á–∞', 'gazprom'],\n",
    "                '–ª—É–∫–æ–π–ª': ['–Ω–µ—Ñ—Ç—å', '–±–µ–Ω–∑–∏–Ω', '—Ç–æ–ø–ª–∏–≤–æ', 'lukoil'],\n",
    "                '–Ω–æ—Ä–∏–ª—å—Å–∫–∏–π –Ω–∏–∫–µ–ª—å': ['–Ω–∏–∫–µ–ª—å', '–º–µ–¥—å', '–º–µ—Ç–∞–ª–ª', 'nornickel'],\n",
    "                '—Ä–æ—Å–Ω–µ—Ñ—Ç—å': ['–Ω–µ—Ñ—Ç—å', '—Ä–æ—Å–Ω–µ—Ñ—Ç–∏', 'rosneft'],\n",
    "                '–∞—ç—Ä–æ—Ñ–ª–æ—Ç': ['–∞–≤–∏–∞', '—Ä–µ–π—Å', '–∞—ç—Ä–æ–ø–æ—Ä—Ç', 'aeroflot'],\n",
    "                '–º–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞': ['–±–∏—Ä–∂–∞', '–º–æ—Å–±–∏—Ä–∂–∞', '—Ç–æ—Ä–≥–∏', 'moex'],\n",
    "                '–≤—Ç–±': ['–≤—Ç–± –±–∞–Ω–∫', 'vtb'],\n",
    "                '–º–∞–≥–Ω–∏—Ç': ['—Ä–∏—Ç–µ–π–ª', '–º–∞–≥–∞–∑–∏–Ω', '—Å–µ—Ç—å –º–∞–≥–∞–∑–∏–Ω–æ–≤', 'magnit'],\n",
    "                '–º—Ç—Å': ['—Å–≤—è–∑—å', '–º–æ–±–∏–ª—å–Ω–∞—è —Å–≤—è–∑—å', 'mts'],\n",
    "                '—Ç–∏–Ω—å–∫–æ—Ñ—Ñ': ['–±–∞–Ω–∫', '–æ–Ω–ª–∞–π–Ω –±–∞–Ω–∫', 'tinkoff'],\n",
    "                '—è–Ω–¥–µ–∫—Å': ['–ø–æ–∏—Å–∫', '–∏–Ω—Ç–µ—Ä–Ω–µ—Ç', 'yandex'],\n",
    "            }\n",
    "            if company in variants:\n",
    "                for variant in variants[company]:\n",
    "                    self.search_dict[variant] = ticker\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º regex –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞\n",
    "        self.pattern = re.compile(\n",
    "            r'\\b(' + '|'.join(map(re.escape, self.search_dict.keys())) + r'|[A-Z]{3,5})\\b',\n",
    "            re.IGNORECASE\n",
    "        )\n",
    "\n",
    "    def find_tickers(self, text: str) -> List[str]:\n",
    "        \"\"\"–ù–∞—Ö–æ–¥–∏—Ç —Ç–∏–∫–µ—Ä—ã –≤ –æ–¥–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return []\n",
    "            \n",
    "        found = set()\n",
    "        for match in self.pattern.findall(text.lower()):\n",
    "            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ\n",
    "            if match in self.search_dict:\n",
    "                found.add(self.search_dict[match])\n",
    "            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –ø—Ä—è–º–æ–π —Ç–∏–∫–µ—Ä\n",
    "            elif match.upper() in self.all_tickers:\n",
    "                found.add(match.upper())\n",
    "                \n",
    "        return list(found)\n",
    "\n",
    "def add_tickers_to_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    text_column: str,\n",
    "    ticker_mapping: Dict[str, str]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ç–∏–∫–µ—Ä–æ–≤ –≤ DataFrame\n",
    "    \"\"\"\n",
    "    extractor = MinimalTickerExtractor(ticker_mapping)\n",
    "    \n",
    "    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ —Ç–∏–∫–µ—Ä–∞–º–∏\n",
    "    df = df.copy()\n",
    "    df['tickers'] = df[text_column].apply(extractor.find_tickers)\n",
    "    return df\n",
    "\n",
    "\n",
    "ticker_mapping = {\n",
    "    '–∞—ç—Ä–æ—Ñ–ª–æ—Ç': 'AFLT',\n",
    "    '–∞–ª—Ä–æ—Å–∞': 'ALRS', \n",
    "    '–≥–∞–∑–ø—Ä–æ–º': 'GAZP',\n",
    "    '–ª—É–∫–æ–π–ª': 'LKOH',\n",
    "    '—Å–±–µ—Ä–±–∞–Ω–∫': 'SBER',\n",
    "    '—Ä–æ—Å–Ω–µ—Ñ—Ç—å': 'ROSN',\n",
    "    '–Ω–æ—Ä–∏–ª—å—Å–∫–∏–π –Ω–∏–∫–µ–ª—å': 'GMKN',\n",
    "    '–º–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞': 'MOEX',\n",
    "    '–≤—Ç–±': 'VTBR',\n",
    "    '–º–∞–≥–Ω–∏—Ç': 'MGNT',\n",
    "    '–º—Ç—Å': 'MTSS',\n",
    "    '—Ç–∞—Ç–Ω–µ—Ñ—Ç—å': 'TATN',\n",
    "    '—Ñ–æ—Å–∞–≥—Ä–æ': 'PHOR',\n",
    "    '–ø–æ–ª—é—Å': 'PLZL',\n",
    "    '—Ä—É—Å–∞–ª': 'RUAL',\n",
    "    '—Å–µ–≤–µ—Ä—Å—Ç–∞–ª—å': 'CHMF',\n",
    "    '–Ω–æ–≤–∞—Ç—ç–∫': 'NVTK'\n",
    "}\n",
    "\n",
    "news_train_with_flags = add_tickers_to_dataframe(\n",
    "    df=news,\n",
    "    text_column='publication',\n",
    "    ticker_mapping=ticker_mapping\n",
    ")\n",
    "\n",
    "\n",
    "class EnhancedNewsAnalyzer:\n",
    "    \"\"\"\n",
    "    –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "        # print(\"‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω\")\n",
    "    \n",
    "    def analyze_sentiment_improved(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return self._neutral_result()\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        positive_indicators = {\n",
    "            '–≤—ã–≥–æ–¥–Ω–æ': 4, '–≤—ã–≥–æ–¥–Ω': 3, '–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤': 3, '–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤': 3,\n",
    "            '—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü': 4, '–ø–æ–∫—É–ø–∞–π': 4, '–∏–Ω–≤–µ—Å—Ç–∏—Ä—É': 3, '–ª–∏–¥–µ—Ä': 3,\n",
    "            '—Ä–µ–∫–æ—Ä–¥': 4, '–ø—Ä–æ—Ä—ã–≤': 4, '—É—Å–ø–µ—à–Ω': 3, '—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω': 2,\n",
    "            \n",
    "            '–ø—Ä–∏–±—ã–ª—å': 3, '–¥–æ—Ö–æ–¥': 3, '–≤—ã—Ä—É—á–∫': 3, '–¥–∏–≤–∏–¥–µ–Ω–¥': 3,\n",
    "            '—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç': 3, '–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç': 3, '–ø—Ä–æ—Ñ–∏—Ü–∏—Ç': 3,\n",
    "            '—Å—Ç–∞–±–∏–ª—å–Ω': 2, '–∫–æ–º—Ñ–æ—Ä—Ç–Ω': 2, '—É–≤–µ—Ä–µ–Ω': 2,\n",
    "            \n",
    "            '—Ä–æ—Å—Ç': 3, '—É–≤–µ–ª–∏—á': 3, '–ø–æ–≤—ã—à': 3, '—É–ª—É—á—à': 3,\n",
    "            '—Ä–∞–∑–≤–∏—Ç': 2, '—Ä–∞—Å—à–∏—Ä–µ–Ω': 2, '–ø—Ä–æ–≥—Ä–µ—Å—Å': 2,\n",
    "            \n",
    "            '–ø–æ–∑–∏—Ç–∏–≤': 3, '–æ–ø—Ç–∏–º–∏–∑–º': 3, '–ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–Ω': 3,\n",
    "            '—Å–∏–ª—å–Ω': 2, '—É—Å–ø–µ—Ö': 3, '–¥–æ—Å—Ç–∏–∂–µ–Ω': 2\n",
    "        }\n",
    "        \n",
    "        negative_indicators = {\n",
    "            '–ø—Ä–æ–±–ª–µ–º': 3, '—Ä–∏—Å–∫': 3, '—É–±—ã—Ç–æ–∫': 4, '–ø–æ—Ç–µ—Ä': 3,\n",
    "            '–∫—Ä–∏–∑–∏—Å': 4, '–æ–ø–∞—Å–Ω': 3, '—É–≥—Ä–æ–∑': 3, '—Å–ª–æ–∂–Ω–æ—Å—Ç': 2,\n",
    "            '—Ç—Ä—É–¥–Ω–æ—Å—Ç': 2, '–Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω': 3, '–≤–æ–ª–∞—Ç–∏–ª—å–Ω': 2,\n",
    "            \n",
    "            '–ø–∞–¥–µ–Ω': 3, '—Å–Ω–∏–∂': 3, '—Å–æ–∫—Ä–∞—â': 3, '—É–º–µ–Ω—å—à': 3,\n",
    "            '—É—Ö—É–¥—à': 3, '–ø—Ä–æ—Å–∞–¥–∫': 3, '–æ–±–≤–∞–ª': 4,\n",
    "            \n",
    "            '–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω': 3, '–Ω–µ–≥–∞—Ç–∏–≤–Ω': 3, '–ø–ª–æ—Ö': 2,\n",
    "            '—Å–ª–∞–±': 2, '–∫—Ä–∏—Ç–∏—á': 3, '–Ω–µ–±–ª–∞–≥–æ–ø—Ä–∏—è—Ç–Ω': 3\n",
    "        }\n",
    "        \n",
    "        intensifiers = {\n",
    "            '–æ—á–µ–Ω—å': 1.5, '–∫—Ä–∞–π–Ω–µ': 2.0, '—Å–∏–ª—å–Ω–æ': 1.5, '–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω': 1.5,\n",
    "            '—Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω': 1.5, '—Ä–µ–∑–∫': 1.5, '–∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á': 2.0, '—Ä–µ–∫–æ—Ä–¥–Ω': 1.5,\n",
    "            '–º–∞—Å—à—Ç–∞–±–Ω': 1.3, '–≤—ã—Å–æ–∫': 1.2, '–±–æ–ª—å—à': 1.1\n",
    "        }\n",
    "        \n",
    "        context_phrases = {\n",
    "            'positive': [\n",
    "                '—Å–º–æ—Ç—Ä—è—Ç—Å—è –≤—ã–≥–æ–¥–Ω–æ', '–º–æ–∂–Ω–æ –æ—Ç–º–µ—Ç–∏—Ç—å', '–æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç–º–µ—Ç–∏–º',\n",
    "                '–∫–æ–º—Ñ–æ—Ä—Ç–Ω–∞—è –¥–æ–ª–≥–æ–≤–∞—è', '–≤—ã—Å–æ–∫–∞—è —Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç—å', '—Å—Ç–∞–±–∏–ª—å–Ω—ã–π –¥–µ–Ω–µ–∂–Ω—ã–π –ø–æ—Ç–æ–∫',\n",
    "                '–≤—ã—Å–æ–∫–∏–µ –¥–∏–≤–∏–¥–µ–Ω–¥—ã', '–¥–∏–≤–∏–¥–µ–Ω–¥–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å', '–ø—Ä–µ–≤—ã—Å–∏—Ç—å 10%'\n",
    "            ],\n",
    "            'negative': [\n",
    "                '—Å–ª–æ–∂–∏–ª—Å—è –Ω–µ–±–ª–∞–≥–æ–ø—Ä–∏—è—Ç–Ω–æ', '—Å–æ–ø—Ä—è–∂–µ–Ω–æ —Å–æ —Å–ª–æ–∂–Ω–æ—Å—Ç—è–º–∏', \n",
    "                '–æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–∏—Å–∫–∏', '—É—Ö—É–¥—à–µ–Ω–∏–µ–º –∫–æ–Ω—ä—é–Ω–∫—Ç—É—Ä—ã'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        positive_score = 0\n",
    "        negative_score = 0\n",
    "        \n",
    "        for word, weight in positive_indicators.items():\n",
    "            if word in text_lower:\n",
    "                count = text_lower.count(word)\n",
    "                for intensifier, multiplier in intensifiers.items():\n",
    "                    if f\"{intensifier} {word}\" in text_lower:\n",
    "                        positive_score += count * weight * multiplier\n",
    "                        break\n",
    "                else:\n",
    "                    positive_score += count * weight\n",
    "        \n",
    "        for word, weight in negative_indicators.items():\n",
    "            if word in text_lower:\n",
    "                count = text_lower.count(word)\n",
    "                for intensifier, multiplier in intensifiers.items():\n",
    "                    if f\"{intensifier} {word}\" in text_lower:\n",
    "                        negative_score += count * weight * multiplier\n",
    "                        break\n",
    "                else:\n",
    "                    negative_score += count * weight\n",
    "        \n",
    "        for phrase in context_phrases['positive']:\n",
    "            if phrase in text_lower:\n",
    "                positive_score += 5\n",
    "        \n",
    "        for phrase in context_phrases['negative']:\n",
    "            if phrase in text_lower:\n",
    "                negative_score += 5\n",
    "        \n",
    "        total_score = positive_score - negative_score\n",
    "        \n",
    "        if total_score >= 2:\n",
    "            sentiment = 'positive'\n",
    "            confidence = min((total_score + 5) / 15, 0.95)\n",
    "        elif total_score <= -3:\n",
    "            sentiment = 'negative'\n",
    "            confidence = min((abs(total_score) + 5) / 15, 0.95)\n",
    "        else:\n",
    "            sentiment = 'neutral'\n",
    "            confidence = 0.6\n",
    "        \n",
    "        emotional_score = total_score / max(positive_score + negative_score + 1, 10)\n",
    "        \n",
    "        return {\n",
    "            'sentiment': sentiment,\n",
    "            'confidence': confidence,\n",
    "            'emotional_score': emotional_score,\n",
    "            'method': 'improved_financial'\n",
    "        }\n",
    "    \n",
    "    def has_financial_context_enhanced(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return False\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        financial_terms = [\n",
    "            '–∞–∫—Ü–∏', '–∞–∫—Ü–∏—è', '–∞–∫—Ü–∏–π', '–¥–∏–≤–∏–¥–µ–Ω–¥', '–∫–æ—Ç–∏—Ä–æ–≤–∫', '–∫—É—Ä—Å', '—Ü–µ–Ω–∞',\n",
    "            '–ø—Ä–∏–±—ã–ª—å', '—É–±—ã—Ç–æ–∫', '–≤—ã—Ä—É—á–∫', '–¥–æ—Ö–æ–¥', '–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç', '–∫–≤–∞—Ä—Ç–∞–ª',\n",
    "            \n",
    "            '–º–ª—Ä–¥', '–º–ª–Ω', '–¥–æ–ª–ª', '—Ä—É–±–ª', '–µ–≤—Ä–æ', '–ø—Ä–æ—Ü–µ–Ω—Ç', '–ø—É–Ω–∫—Ç',\n",
    "            \n",
    "            '—Ä–æ—Å—Ç', '–ø–∞–¥–µ–Ω–∏', '–∏–Ω–≤–µ—Å—Ç', '–ø–æ—Ä—Ç—Ñ–µ–ª—å', '—Ä—ã–Ω–æ–∫', '–±–∏—Ä–∂',\n",
    "            '—Å–¥–µ–ª–∫–∞', '–ø–æ–∫—É–ø–∫', '–ø—Ä–æ–¥–∞–∂', '–≤–æ–ª–∞—Ç–∏–ª—å–Ω', '–ª–∏–∫–≤–∏–¥–Ω–æ—Å—Ç',\n",
    "            \n",
    "            '—Ñ–∏–Ω–∞–Ω—Å', '–±—é–¥–∂–µ—Ç', '–∫–∞–ø–∑–∞—Ç—Ä–∞—Ç', '–∞–∫—Ç–∏–≤', '–ø–∞—Å—Å–∏–≤', '–±–∞–ª–∞–Ω—Å',\n",
    "            '–≤—ã–ø–ª–∞—Ç', '–∑–∞—Ç—Ä–∞—Ç', '–∏–∑–¥–µ—Ä–∂–∫', '—Ä–µ–Ω—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç', '–¥–æ—Ö–æ–¥–Ω–æ—Å—Ç',\n",
    "            \n",
    "            '–¥–æ–ª–≥–æ–≤', '–Ω–∞–≥—Ä—É–∑–∫', '–¥–µ–Ω–µ–∂–Ω', '–ø–æ—Ç–æ–∫', 'fcf', 'ebitda',\n",
    "            '–º—É–ª—å—Ç–∏–ø–ª–∏–∫–∞—Ç–æ—Ä', '–∫–æ–Ω—ä—é–Ω–∫—Ç—É—Ä', '–Ω–∞–ª–æ–≥–æ–æ–±–ª–æ–∂–µ–Ω', '—ç–º–∏—Ç–µ–Ω—Ç',\n",
    "            '—ç–º–∏—Å—Å–∏', '–∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è', '–æ–±–ª–∏–≥–∞—Ü', '–∫—É–ø–æ–Ω',\n",
    "            \n",
    "            '–∫—Ä–µ–¥–∏—Ç', '–∑–∞–µ–º', '–¥–µ–ø–æ–∑–∏—Ç', '–≤–∫–ª–∞–¥', '–∏–ø–æ—Ç–µ–∫', '—Ä–µ—Ñ–∏–Ω–∞–Ω—Å',\n",
    "            \n",
    "            '–≤–≤–ø', '–∏–Ω—Ñ–ª—è—Ü', '–∫–ª—é—á–µ–≤', '—Å—Ç–∞–≤–∫', '—Ü–±', '—Ü–µ–Ω—Ç—Ä–æ–±–∞–Ω–∫',\n",
    "            \n",
    "            '–Ω–µ—Ñ—Ç', '–≥–∞–∑', '—ç–Ω–µ—Ä–≥', '–º–µ—Ç–∞–ª', '–≥–æ—Ä–Ω–æ–¥–æ–±—ã–≤–∞', '—Ç–µ–ª–µ–∫–æ–º',\n",
    "            '—Ä–∏—Ç–µ–π–ª', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç', '—Ö–∏–º–∏'\n",
    "        ]\n",
    "        \n",
    "        financial_phrases = [\n",
    "            '–¥–µ–Ω–µ–∂–Ω—ã–π –ø–æ—Ç–æ–∫', '–¥–æ–ª–≥–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞', '—á–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å',\n",
    "            '–≤–∞–ª–æ–≤–∞—è –≤—ã—Ä—É—á–∫–∞', '–æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å', '—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç',\n",
    "            '–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å –ø–æ –º—Å—Ñ–æ', '–¥–∏–≤–∏–¥–µ–Ω–¥–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞', '—Ä—ã–Ω–æ—á–Ω–∞—è –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏—è',\n",
    "            '–∫–æ—Ç–∏—Ä–æ–≤–∫–∏ –∞–∫—Ü–∏–π', '–±–∏—Ä–∂–µ–≤—ã–µ —Ç–æ—Ä–≥–∏', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π –ø–æ—Ä—Ç—Ñ–µ–ª—å',\n",
    "            '—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑', '—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å', '–º–∞–∫—Ä–æ—ç–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è —Å–∏—Ç—É–∞—Ü–∏—è'\n",
    "        ]\n",
    "        \n",
    "        financial_terms_count = 0\n",
    "        \n",
    "        for term in financial_terms:\n",
    "            if term in text_lower:\n",
    "                financial_terms_count += 1\n",
    "        \n",
    "        for phrase in financial_phrases:\n",
    "            if phrase in text_lower:\n",
    "                financial_terms_count += 3\n",
    "        \n",
    "        financial_contexts = [\n",
    "            r'\\b(?:–æ—Ç—á–µ—Ç|–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å|–±–∞–ª–∞–Ω—Å|–ø—Ä–∏–±—ã–ª—å|—É–±—ã—Ç–æ–∫)[^.]{0,100}',\n",
    "            r'\\b(?:—Ü–µ–Ω–∞|–∫—É—Ä—Å|–∫–æ—Ç–∏—Ä–æ–≤–∫–∏)[^.]{0,100}(?:–∞–∫—Ü–∏|–∞–∫—Ü–∏–π|–æ–±–ª–∏–≥–∞—Ü)',\n",
    "            r'\\b(?:–¥–∏–≤–∏–¥–µ–Ω–¥|–≤—ã–ø–ª–∞—Ç–∞)[^.]{0,100}(?:–∞–∫—Ü–∏–æ–Ω–µ—Ä|–ø—Ä–∏–±—ã–ª—å)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in financial_contexts:\n",
    "            if re.search(pattern, text_lower):\n",
    "                financial_terms_count += 2\n",
    "        \n",
    "        return financial_terms_count >= 3\n",
    "    \n",
    "    def _neutral_result(self):\n",
    "        return {\n",
    "            'sentiment': 'neutral',\n",
    "            'confidence': 0.5,\n",
    "            'emotional_score': 0.0,\n",
    "            'method': 'fallback'\n",
    "        }\n",
    "    \n",
    "    def analyze_news_comprehensive(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–æ—Å—Ç–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏\n",
    "        \"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return {\n",
    "                'sentiment': 'neutral',\n",
    "                'confidence': 0.0,\n",
    "                'emotional_score': 0.0,\n",
    "                'has_financial_context': False,\n",
    "                'method': 'fallback'\n",
    "            }\n",
    "        \n",
    "        sentiment_result = self.analyze_sentiment_improved(text)\n",
    "        has_financial_context = self.has_financial_context_enhanced(text)\n",
    "        \n",
    "        return {\n",
    "            **sentiment_result,\n",
    "            'has_financial_context': has_financial_context\n",
    "        }\n",
    "\n",
    "def extract_sources_advanced(text: str) -> str:\n",
    "    \"\"\"\n",
    "    –£–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLP\n",
    "    \"\"\"\n",
    "    extractor = SourceExtractor()\n",
    "    return extractor.extract_sources_advanced(text)\n",
    "\n",
    "def reanalyze_with_enhancements(df) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "    \"\"\"\n",
    "    analyzer = EnhancedNewsAnalyzer()\n",
    "        \n",
    "    analysis_results = df['publication'].apply(analyzer.analyze_news_comprehensive)\n",
    "    \n",
    "    df['sentiment'] = analysis_results.apply(lambda x: x['sentiment'])\n",
    "    df['sentiment_confidence'] = analysis_results.apply(lambda x: x['confidence'])\n",
    "    df['emotional_score'] = analysis_results.apply(lambda x: x['emotional_score'])\n",
    "    df['analysis_method'] = analysis_results.apply(lambda x: x['method'])\n",
    "    df['has_financial_context'] = analysis_results.apply(lambda x: x['has_financial_context'])\n",
    "    # df['source'] = df['publication'].apply(extract_sources_advanced) # –ø–æ–∫–∞ –Ω–µ —Ä–∞–±–æ—á–∞—è –∏—Å—Ç–æ—Ä–∏—è\n",
    "    \n",
    "    # print(\"‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "news_features = reanalyze_with_enhancements(news_train_with_flags)\n",
    "\n",
    "\n",
    "\n",
    "def build_ticker_aliases(tickers):\n",
    "    base = {\n",
    "        'AFLT': ['–∞—ç—Ä–æ—Ñ–ª–æ—Ç', 'aeroflot'],\n",
    "        'ALRS': ['–∞–ª—Ä–æ—Å–∞', 'alrosa'],\n",
    "        'CHMF': ['—Å–µ–≤–µ—Ä—Å—Ç–∞–ª—å', 'severstal'],\n",
    "        'GAZP': ['–≥–∞–∑–ø—Ä–æ–º', 'gazprom'],\n",
    "        'GMKN': ['–Ω–æ—Ä–Ω–∏–∫–µ–ª—å', '–Ω–æ—Ä–∏–ª—å—Å–∫–∏–π –Ω–∏–∫–µ–ª—å', 'nornickel', 'norilsk nickel'],\n",
    "        'LKOH': ['–ª—É–∫–æ–π–ª', 'lukoil'],\n",
    "        'MAGN': ['–º–º–∫', '–º–∞–≥–Ω–∏—Ç–æ–≥–æ—Ä—Å–∫–∏–π –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–∏–π –∫–æ–º–±–∏–Ω–∞—Ç', 'mmk'],\n",
    "        'MGNT': ['–º–∞–≥–Ω–∏—Ç', 'magnit'],\n",
    "        'MOEX': ['–º–æ—Å–±–∏—Ä–∂–∞', '–º–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞', 'moex'],\n",
    "        'MTSS': ['–º—Ç—Å', 'mts'],\n",
    "        'NVTK': ['–Ω–æ–≤–∞—Ç—ç–∫', 'novatek'],\n",
    "        'PHOR': ['—Ñ–æ—Å–∞–≥—Ä–æ', 'phosagro'],\n",
    "        'PLZL': ['–ø–æ–ª—é—Å', 'polyus'],\n",
    "        'ROSN': ['—Ä–æ—Å–Ω–µ—Ñ—Ç—å', 'rosneft'],\n",
    "        'RUAL': ['—Ä—É—Å–∞–ª', 'rusal'],\n",
    "        'SBER': ['—Å–±–µ—Ä', '—Å–±–µ—Ä–±–∞–Ω–∫', 'sber', 'sberbank'],\n",
    "        'SIBN': ['–≥–∞–∑–ø—Ä–æ–º –Ω–µ—Ñ—Ç—å', 'gazprom neft'],\n",
    "        'T':    ['—Ç–∏–Ω—å–∫–æ—Ñ—Ñ', '—Ç-–±–∞–Ω–∫', 'tinkoff', 't-bank'],\n",
    "        'VTBR': ['–≤—Ç–±', 'vtb'],\n",
    "    }\n",
    "    return {t: list(set(base.get(t, []) + [t.lower()])) for t in tickers}\n",
    "\n",
    "def preprocess_sentiment_dummies(news_df):\n",
    "    \"\"\"–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º sentiment –≤ one-hot encoding\"\"\"\n",
    "    df = news_df.copy()\n",
    "    \n",
    "    if 'sentiment' in df.columns:\n",
    "        sentiment_dummies = pd.get_dummies(df['sentiment'], prefix='sentiment')\n",
    "        df = pd.concat([df, sentiment_dummies], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def tag_news_with_tickers(news_df, ticker_aliases, text_cols=('title','publication')):\n",
    "    news = news_df.copy()\n",
    "    news['publish_date'] = pd.to_datetime(news['publish_date'])\n",
    "    news = preprocess_sentiment_dummies(news)\n",
    "    \n",
    "    mentioned_cols = [col for col in news.columns if col.startswith('mentioned_')]\n",
    "    \n",
    "    if mentioned_cols:\n",
    "        def get_mentioned_tickers(row):\n",
    "            tickers_found = []\n",
    "            for col in mentioned_cols:\n",
    "                if row[col] == 1:\n",
    "                    ticker = col.replace('mentioned_', '')\n",
    "                    tickers_found.append(ticker)\n",
    "            return tickers_found\n",
    "        \n",
    "        news['tickers'] = news.apply(get_mentioned_tickers, axis=1)\n",
    "    else:\n",
    "        news['_text'] = ''\n",
    "        for c in text_cols:\n",
    "            if c in news.columns:\n",
    "                news['_text'] = (news['_text'] + ' ' + news[c].astype(str)).str.lower()\n",
    "        \n",
    "        def find_tickers(text):\n",
    "            if not isinstance(text, str) or not text:\n",
    "                return []\n",
    "            found = []\n",
    "            for tkr, keys in ticker_aliases.items():\n",
    "                if any(k and k in text for k in keys):\n",
    "                    found.append(tkr)\n",
    "            return list(set(found))\n",
    "        \n",
    "        news['tickers'] = news['_text'].apply(find_tickers)\n",
    "        news = news.drop(columns=['_text'])\n",
    "    \n",
    "    return news\n",
    "\n",
    "def aggregate_news_features_by_ticker(news_tagged):\n",
    "    df = news_tagged.copy()\n",
    "    df['date'] = df['publish_date'].dt.normalize()\n",
    "\n",
    "    exploded = df.explode('tickers')\n",
    "    exploded = exploded[exploded['tickers'].notna() & (exploded['tickers'] != '')]\n",
    "\n",
    "    sentiment_cols = [col for col in exploded.columns if col.startswith('sentiment_')]\n",
    "\n",
    "    aggregation_dict = {\n",
    "        'title': 'count',\n",
    "    }\n",
    "    if 'sentiment_confidence' in exploded.columns:\n",
    "        aggregation_dict['sentiment_confidence'] = 'mean'\n",
    "    if 'emotional_score' in exploded.columns:\n",
    "        aggregation_dict['emotional_score'] = 'mean'\n",
    "    if 'has_financial_context' in exploded.columns:\n",
    "        aggregation_dict['has_financial_context'] = 'sum'\n",
    "\n",
    "    for col in sentiment_cols:\n",
    "        aggregation_dict[col] = 'sum'\n",
    "\n",
    "    agg = exploded.groupby(['tickers', 'date']).agg(aggregation_dict).reset_index()\n",
    "\n",
    "    rename_map = {\n",
    "        'tickers': 'ticker',\n",
    "        'title': 'news_count',\n",
    "        'sentiment_confidence': 'avg_sentiment_confidence',\n",
    "        'emotional_score': 'avg_emotional_score',\n",
    "        'has_financial_context': 'financial_news_count'\n",
    "    }\n",
    "    agg = agg.rename(columns=rename_map)\n",
    "\n",
    "    if 'financial_news_count' in agg.columns:\n",
    "        agg['financial_news_ratio'] = agg['financial_news_count'] / agg['news_count']\n",
    "    else:\n",
    "        agg['financial_news_count'] = 0.0\n",
    "        agg['financial_news_ratio'] = 0.0\n",
    "\n",
    "    for col in sentiment_cols:\n",
    "        if col in agg.columns:\n",
    "            ratio_col = f'{col}_ratio'\n",
    "            agg[ratio_col] = agg[col] / agg['news_count']\n",
    "\n",
    "    agg = agg.fillna(0.0)\n",
    "\n",
    "    return agg\n",
    "\n",
    "def aggregate_news_counts_by_ticker(news_tagged):\n",
    "    df = news_tagged.copy()\n",
    "    df['date'] = df['publish_date'].dt.normalize()\n",
    "    exploded = df.explode('tickers')\n",
    "    exploded = exploded[exploded['tickers'].notna() & (exploded['tickers']!='')]\n",
    "    agg = exploded.groupby(['tickers','date']).size().reset_index(name='news_count')\n",
    "    agg = agg.rename(columns={'tickers':'ticker'})\n",
    "\n",
    "    \n",
    "    return agg\n",
    "\n",
    "def add_news_features_by_ticker(candles_df, news_df, ticker_aliases):\n",
    "    candles = candles_df.copy()\n",
    "    candles['begin'] = pd.to_datetime(candles['begin'])\n",
    "    candles['date'] = candles['begin'].dt.normalize()\n",
    "    \n",
    "    if news_df is None or len(news_df)==0:\n",
    "        candles['news_count'] = 0.0\n",
    "        candles['avg_sentiment_confidence'] = 0.0\n",
    "        candles['avg_emotional_score'] = 0.0\n",
    "        candles['financial_news_count'] = 0.0\n",
    "        candles['financial_news_ratio'] = 0.0\n",
    "        \n",
    "        return candles.drop(columns=['date'])\n",
    "    \n",
    "    tagged = tag_news_with_tickers(news_df, ticker_aliases)\n",
    "    news_features = aggregate_news_features_by_ticker(tagged)\n",
    "    news_features['date'] += pd.Timedelta(days=1)\n",
    "    \n",
    "    out = candles.merge(news_features, on=['ticker','date'], how='left')\n",
    "    \n",
    "    news_cols = [col for col in news_features.columns if col not in ['ticker', 'date']]\n",
    "    \n",
    "    for col in news_cols:\n",
    "        out[col] = out[col].fillna(0.0)\n",
    "    \n",
    "    expected_sentiments = ['sentiment_positive', 'sentiment_negative', 'sentiment_neutral', 'sentiment_mixed']\n",
    "    for sent in expected_sentiments:\n",
    "        if sent not in out.columns:\n",
    "            out[sent] = 0.0\n",
    "        if f'{sent}_ratio' not in out.columns:\n",
    "            out[f'{sent}_ratio'] = 0.0\n",
    "    \n",
    "    return out.drop(columns=['date'])\n",
    "\n",
    "def add_news_count_by_ticker(candles_df, news_df, ticker_aliases):\n",
    "    candles = candles_df.copy()\n",
    "    candles['begin'] = pd.to_datetime(candles['begin'])\n",
    "    candles['date'] = candles['begin'].dt.normalize()\n",
    "    if news_df is None or len(news_df)==0:\n",
    "        candles['news_count'] = 0.0\n",
    "        return candles.drop(columns=['date'])\n",
    "    tagged = tag_news_with_tickers(news_df, ticker_aliases)\n",
    "    per_ticker = aggregate_news_counts_by_ticker(tagged)\n",
    "    out = candles.merge(per_ticker, on=['ticker','date'], how='left')\n",
    "    out['news_count'] = out['news_count'].fillna(0.0)\n",
    "    return out.drop(columns=['date'])\n",
    "\n",
    "FEATS = [\n",
    "    'momentum_5', 'volatility_5', 'price_range',\n",
    "    'news_count', 'financial_news_count', 'financial_news_ratio',\n",
    "    'avg_sentiment_confidence', 'avg_emotional_score',\n",
    "    'sentiment_positive', 'sentiment_negative', 'sentiment_neutral', 'sentiment_mixed',\n",
    "    'sentiment_positive_ratio', 'sentiment_negative_ratio', 'sentiment_neutral_ratio', 'sentiment_mixed_ratio'\n",
    "]\n",
    "\n",
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['begin'] = pd.to_datetime(df['begin'])\n",
    "    df = df.sort_values(['ticker','begin']).reset_index(drop=True)\n",
    "    df['momentum_5'] = df.groupby('ticker')['close'].pct_change(5).shift(1)\n",
    "    ret1 = df.groupby('ticker')['close'].pct_change()\n",
    "    df['volatility_5'] = ret1.groupby(df['ticker']).rolling(5, min_periods=1).std().reset_index(level=0, drop=True)\n",
    "    df['price_range'] = (df['high'] - df['low'])/df['close']\n",
    "    \n",
    "    for c in ['momentum_5','volatility_5','price_range']:\n",
    "        df[c] = df[c].fillna(0.0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_targets(df, horizons=(1,20)):\n",
    "    out = df.copy()\n",
    "    for h in horizons:\n",
    "        out[f'target_return_{h}d'] = out.groupby('ticker')['close'].pct_change(h).shift(-h)\n",
    "    return out\n",
    "\n",
    "def fit(candles_train, news_train=None, split_date='2024-09-08', model_path='model.pkl'):\n",
    "    df = candles_train.copy()\n",
    "    df['begin'] = pd.to_datetime(df['begin'])\n",
    "    cutoff = pd.to_datetime(split_date)\n",
    "    df = df[df['begin'] <= cutoff].copy()\n",
    "\n",
    "    if len(df) == 0:\n",
    "        raise ValueError(\"Train slice after split_date is empty; adjust split_date or inputs.\")\n",
    "\n",
    "    aliases = build_ticker_aliases(sorted(df['ticker'].unique()))\n",
    "\n",
    "    df = create_features(df)\n",
    "    df = add_news_features_by_ticker(df, news_train, aliases)\n",
    "    df = create_targets(df, horizons=(1, 20))\n",
    "\n",
    "    mask = ~df[[f'target_return_{h}d' for h in (1, 20)]].isna().any(axis=1)\n",
    "    dft = df.loc[mask].reset_index(drop=True)\n",
    "\n",
    "    if len(dft) == 0:\n",
    "        raise ValueError(\"No valid rows with both targets h=1 and h=20 in train.\")\n",
    "\n",
    "    # X, y\n",
    "    X = dft.drop(columns=['begin', 'ticker'] + [f'target_return_{h}d' for h in (1, 20)]).values\n",
    "    feature_names = [c for c in dft.columns if c not in ['begin', 'ticker',\n",
    "                                                        'target_return_1d', 'target_return_20d']]\n",
    "\n",
    "    models = {}\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    for h in (1, 20):\n",
    "        y = dft[f'target_return_{h}d'].values\n",
    "\n",
    "        reg = CatBoostRegressor(\n",
    "            iterations=1000,\n",
    "            depth=6,\n",
    "            learning_rate=0.05,\n",
    "            loss_function='RMSE',\n",
    "            random_seed=42,\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        scores = []\n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "            Xtr, Xval = X[train_idx], X[val_idx]\n",
    "            ytr, yval = y[train_idx], y[val_idx]\n",
    "            reg.fit(Xtr, ytr, eval_set=(Xval, yval), use_best_model=True)\n",
    "            preds = reg.predict(Xval)\n",
    "            fold_rmse = np.sqrt(np.mean((preds - yval) ** 2))\n",
    "            scores.append(fold_rmse)\n",
    "            # print(f\"[h={h}d] Fold {fold+1}: RMSE={fold_rmse:.4f}\")\n",
    "\n",
    "        # print(f\"=== Horizon {h}d: CV mean RMSE={np.mean(scores):.4f} ¬± {np.std(scores):.4f} ===\")\n",
    "        models[f'reg_{h}'] = reg\n",
    "\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'features': feature_names,\n",
    "            'models': models,\n",
    "            'aliases': aliases,\n",
    "            'split_date': str(split_date)\n",
    "        }, f)\n",
    "\n",
    "def _safe_write_csv(df, path):\n",
    "    df.to_csv(path, index=False, sep=',', encoding='utf-8-sig', lineterminator='\\n')  # —Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –∑–∞–ø–∏—Å—å CSV [web:159]\n",
    "\n",
    "def predict_on_cutoff(candles_test, news_test=None, model_path='model.pkl', output_path='submission.csv'):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        payload = pickle.load(f)\n",
    "        \n",
    "    feats   = payload['features']\n",
    "    models  = payload['models']\n",
    "    aliases = payload['aliases']\n",
    "    split_date = pd.to_datetime(payload['split_date'])\n",
    "    \n",
    "    df = candles_test.copy()\n",
    "    df['begin'] = pd.to_datetime(df['begin'])\n",
    "    df = df[df['begin'] > split_date].copy()\n",
    "    \n",
    "    df = create_features(df)\n",
    "    df = add_news_features_by_ticker(df, news_test, aliases)\n",
    "    \n",
    "    X = df[feats].values\n",
    "    \n",
    "    for h in (1, 20):\n",
    "        reg = models[f'reg_{h}']\n",
    "        df[f'pred_return_{h}d'] = reg.predict(X)\n",
    "    \n",
    "    out = df[['ticker', 'begin', 'pred_return_1d', 'pred_return_20d']]\n",
    "    out.to_csv(output_path, index=False)\n",
    "    # print(f\"‚úÖ Predictions saved to {output_path}\")\n",
    "    return out\n",
    "\n",
    "def predict_for_date(candles_df, news_df, model_path, prediction_date, output_path='submission.csv'):\n",
    "    \"\"\"\n",
    "    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –¥–∞—Ç—É\n",
    "    \n",
    "    Args:\n",
    "        candles_df: DataFrame —Å –∫–æ—Ç–∏—Ä–æ–≤–∫–∞–º–∏\n",
    "        news_df: DataFrame —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏\n",
    "        model_path: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "        prediction_date: –¥–∞—Ç–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ (—Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ datetime)\n",
    "        output_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        payload = pickle.load(f)\n",
    "        \n",
    "    feats = payload['features']\n",
    "    models = payload['models']\n",
    "    aliases = payload['aliases']\n",
    "    \n",
    "    prediction_date = pd.to_datetime(prediction_date)\n",
    "    \n",
    "    df = candles_df.copy()\n",
    "    df['begin'] = pd.to_datetime(df['begin'])\n",
    "    \n",
    "    df = df[df['begin'] == prediction_date].copy()\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        raise ValueError(f\"No data available for prediction date {prediction_date}\")\n",
    "    \n",
    "    df = create_features(df)\n",
    "    df = add_news_features_by_ticker(df, news_df, aliases)\n",
    "    \n",
    "    missing_features = set(feats) - set(df.columns)\n",
    "    if missing_features:\n",
    "        # print(f\"‚ö†Ô∏è Adding missing features: {missing_features}\")\n",
    "        for feature in missing_features:\n",
    "            df[feature] = 0.0\n",
    "    \n",
    "    X = df[feats].values\n",
    "    tickers = df['ticker'].values\n",
    "    \n",
    "    predictions = {}\n",
    "    for h in range(1, 21):\n",
    "        model_key = f'reg_{h}'\n",
    "        if model_key in models:\n",
    "            predictions[f'p{h}'] = models[model_key].predict(X)\n",
    "        else:\n",
    "            available_horizons = [int(k.split('_')[1]) for k in models.keys() if k.startswith('reg_')]\n",
    "            closest_h = min(available_horizons, key=lambda x: abs(x - h))\n",
    "            predictions[f'p{h}'] = models[f'reg_{closest_h}'].predict(X)\n",
    "            # print(f\"‚ö†Ô∏è Using model for horizon {closest_h} for horizon {h}\")\n",
    "    \n",
    "    result_df = pd.DataFrame({'ticker': tickers})\n",
    "    for h in range(1, 21):\n",
    "        result_df[f'p{h}'] = predictions[f'p{h}']\n",
    "    \n",
    "    result_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "fit(candles, news_features, split_date='2024-09-08', model_path='model.pkl')\n",
    "# predict_on_cutoff(candles, news_features, model_path='model.pkl', output_path='submission.csv')\n",
    "\n",
    "results = predict_for_date(\n",
    "    candles_df=candles,\n",
    "    news_df=news_features, \n",
    "    model_path='model.pkl',\n",
    "    prediction_date='2024-09-09',\n",
    "    output_path='submission.csv'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d5795f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "print(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0680058a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
